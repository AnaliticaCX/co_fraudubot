{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subfolder 'docs' does not exist in the current directory: /Users/carloscuartas/Library/CloudStorage/Box-Box/MM-TI-Análitica/03. Proyectos/Fraudubot Final/Notebooks/docs\n",
      "\n",
      "Extracted DataFrame:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "from pdf2image import convert_from_path\n",
    "from pytesseract import image_to_string\n",
    "from PIL import Image\n",
    "\n",
    "# Load spaCy's Spanish NER model\n",
    "#python -m spacy download es_core_news_lg\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "# Tesseract OCR setup (Ensure Tesseract is installed and path is correct)\n",
    "TESSDATA_PREFIX = \"/usr/share/tesseract-ocr/4.00/tessdata\"\n",
    "os.environ[\"TESSDATA_PREFIX\"] = TESSDATA_PREFIX\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    \"\"\"\n",
    "    Extract all text from a Word document.\n",
    "    \"\"\"\n",
    "    doc = Document(file_path)\n",
    "    return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file using OCR.\n",
    "    Converts each page of the PDF to an image and extracts text using Tesseract.\n",
    "    \"\"\"\n",
    "    pages = convert_from_path(file_path)\n",
    "    text = \"\"\n",
    "    for page in pages:\n",
    "        text += image_to_string(page, lang=\"spa\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_text_from_png(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PNG file using OCR.\n",
    "    \"\"\"\n",
    "    image = Image.open(file_path)\n",
    "    return image_to_string(image, lang=\"spa\")\n",
    "\n",
    "def remove_accents(text):\n",
    "    return ''.join(\n",
    "        (c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "    )\n",
    "\n",
    "def extract_cc(text):\n",
    "    \"\"\"\n",
    "    Extract CC (Cédula de Ciudadanía) from the given text.\n",
    "    \"\"\"\n",
    "    # Normalize the text by removing accents\n",
    "    text_normalized = remove_accents(text).lower()\n",
    "    print(f\"Normalized Text: {text_normalized}\")  # Debugging\n",
    "\n",
    "    # Enhanced regex to handle different formats\n",
    "    cc_match = re.search(\n",
    "        r\"(?:cc|cedula(?: de ciudadania)?|cedula|ciudadania|documento)(?:\\s*(?:numero|num|no\\.?|no:|número|#)?\\s*[:\\s]*)?([\\d\\s\\-\\.]{6,})\",\n",
    "        text_normalized,\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "    if cc_match:\n",
    "        # Extract the raw CC part from the match\n",
    "        cc_raw = cc_match.group(1)\n",
    "        print(f\"Raw CC Match: {cc_raw}\")  # Debugging\n",
    "        \n",
    "        # Clean up to remove non-digit characters\n",
    "        cc_cleaned = re.sub(r\"[^\\d]\", \"\", cc_raw)\n",
    "        print(f\"Cleaned CC: {cc_cleaned}\")  # Debugging\n",
    "\n",
    "        # Return if it meets a reasonable length criterion\n",
    "        if len(cc_cleaned) >= 6:  # Adjust the minimum length if needed\n",
    "            return cc_cleaned\n",
    "\n",
    "    # Return None if no match is found\n",
    "    return None\n",
    "\n",
    "def extract_name(text):\n",
    "    \"\"\"\n",
    "    Extracts a person's name from the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted name or None if no name is found.\n",
    "    \"\"\"\n",
    "    # Common words or titles that should be removed before extracting the name\n",
    "    stop_words = {\"la señora\", \"el señor\", \"señor\", \"señora\", \"la\", \"el\", \"El\", \"La\", \"identificado\", \"con\", \"cedula\", \"de\", \"ciudadania\"}\n",
    "\n",
    "    # Step 1: Normalize and clean the text\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # Step 2: Use SpaCy's Named Entity Recognition to find the person\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PER\":  # Look for \"Person\" entities\n",
    "            # Clean the name by removing any stop words (like titles)\n",
    "            cleaned_name = \" \".join(\n",
    "                word for word in ent.text.split() if word.lower() not in stop_words\n",
    "            )\n",
    "            return cleaned_name.strip()\n",
    "\n",
    "    # Step 3: Regex fallback for structured patterns like \"el señor [NAME]\"\n",
    "    # This regex looks for common patterns in Spanish like \"señor [NAME]\" or \"identificado con cédula de [NAME]\"\n",
    "    context_pattern = r\"(?i)(?:el\\s+señor|la\\s+señora|señor(?:a)?|identificado\\s+con\\s+c[eé]dula\\s+de)\\s+([A-ZÁÉÍÓÚÑ][A-Za-zÁÉÍÓÚÑ]*(?:\\s+[A-ZÁÉÍÓÚÑ][A-Za-zÁÉÍÓÚÑ]*)+)\"\n",
    "    match = re.search(context_pattern, text)\n",
    "    if match:\n",
    "        candidate_name = match.group(1).strip()\n",
    "        return candidate_name.title()\n",
    "\n",
    "    # Step 4: Return None if no name is found\n",
    "    return None\n",
    "\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import dateparser\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import dateparser\n",
    "\n",
    "def extract_tiempo_laborado(text):\n",
    "    \"\"\"\n",
    "    Extracts the start and end dates of a working period from text.\n",
    "    Handles phrases like \"desde [day] [month] [year]\" and \"hasta [day] [month] [year]\".\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text containing work details.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (from_date, to_date) in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    # Extract the current date\n",
    "    current_date = datetime.now()\n",
    "\n",
    "    # Initialize from_date and to_date\n",
    "    from_date, to_date = None, None\n",
    "\n",
    "    # Extract the start date using regex for the pattern \"desde el día [day] [month] [year]\" or \"desde [day] [month] [year]\"\n",
    "    desde_match = re.search(r\"desde\\s+el?\\s*(día\\s*)?(\\d{1,2})\\s*de\\s*(\\w+)\\s*de\\s*(\\d{4})\", text, re.IGNORECASE)\n",
    "    \n",
    "    if desde_match:\n",
    "        # Extract day, month, and year\n",
    "        day = desde_match.group(2)\n",
    "        month = desde_match.group(3)\n",
    "        year = desde_match.group(4)\n",
    "        \n",
    "        # Format the start date as 'YYYY-MM-DD'\n",
    "        from_date = dateparser.parse(f\"{day} {month} {year}\", languages=['es']).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Extract the end date using regex for the pattern \"hasta el [day] [month] [year]\"\n",
    "    hasta_match = re.search(r\"hasta\\s+el?\\s*(\\d{1,2})\\s*de\\s*(\\w+)\\s*de\\s*(\\d{4})\", text, re.IGNORECASE)\n",
    "    \n",
    "    if hasta_match:\n",
    "        # Extract day, month, and year\n",
    "        day = hasta_match.group(1)\n",
    "        month = hasta_match.group(2)\n",
    "        year = hasta_match.group(3)\n",
    "        \n",
    "        # Format the end date as 'YYYY-MM-DD'\n",
    "        to_date = dateparser.parse(f\"{day} {month} {year}\", languages=['es']).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Handle \"hasta hoy\" or similar phrases\n",
    "    present_terms = [\"hasta hoy\", \"hoy\", \"actualidad\", \"presente\", \"hasta la fecha\"]\n",
    "    if any(term in text.lower() for term in present_terms):\n",
    "        to_date = current_date.strftime('%Y-%m-%d')  # Set \"to_date\" to today's date\n",
    "    \n",
    "    # Fallback if no \"hasta\" date was found, default to the current date\n",
    "    if not to_date:\n",
    "        to_date = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # If no \"desde\" date was found, attempt to use the first year mentioned\n",
    "    if not from_date:\n",
    "        year_match = re.search(r\"(\\d{4})\", text)\n",
    "        if year_match:\n",
    "            from_date = f\"{year_match.group(1)}-01-01\"  # Default to January 1st of the year\n",
    "\n",
    "    return from_date, to_date\n",
    "\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_name(text):\n",
    "    \"\"\"\n",
    "    Extracts a person's name from the text after removing common titles and roles.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted name or None if no name is found.\n",
    "    \"\"\"\n",
    "    # Define stop words (titles, roles, etc.)\n",
    "    stop_words = {\n",
    "        \"la señora\", \"el señor\", \"señor\", \"señora\", \"la\", \"el\", \"El\", \"La\",\n",
    "        \"identificado(a)\", \"certifica\", \"que\", \"director\", \"directora\",\n",
    "        \"coordinador\", \"coordinadora\", \"gerente\", \"jefe\", \"jefa\", \"cargo\", \"responsable\", \"empleado\"\n",
    "    }\n",
    "\n",
    "    # Step 1: Clean the text by removing unwanted words\n",
    "    cleaned_text = text\n",
    "    for word in stop_words:\n",
    "        cleaned_text = re.sub(r'\\b' + re.escape(word) + r'\\b', '', cleaned_text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 2: Use SpaCy's Named Entity Recognition (NER) to extract person names\n",
    "    doc = nlp(cleaned_text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PER\":  # Look for \"Person\" entities\n",
    "            return ent.text.strip()\n",
    "\n",
    "    # Step 3: Regex fallback for structured patterns like \"el señor [NAME]\"\n",
    "    context_pattern = r\"(?i)(?:señor(?:a)?\\s+)([A-ZÁÉÍÓÚÑ][A-Za-zÁÉÍÓÚÑ]*(?:\\s+[A-ZÁÉÍÓÚÑ][A-Za-zÁÉÍÓÚÑ]*)+)\"\n",
    "    match = re.search(context_pattern, cleaned_text)\n",
    "    if match:\n",
    "        candidate_name = match.group(1).strip()\n",
    "        return candidate_name.title()\n",
    "\n",
    "    # Step 4: Return None if no match is found\n",
    "    return None\n",
    "\n",
    "def extract_salario(text):\n",
    "    \"\"\"\n",
    "    Extract salary information from the given text using regex.\n",
    "    Removes all non-numeric characters from the salary value.\n",
    "    \"\"\"\n",
    "    # Regex to capture salary patterns\n",
    "    pattern = r\"(?:salario\\s*[:de]*\\s*)([\\d.,]+)|[\\$€]\\s?[\\d.,]+\"\n",
    "    \n",
    "    # Search for the first match\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        # Extract the numeric portion\n",
    "        salary_str = match.group(1) if match.group(1) else match.group(0)\n",
    "        \n",
    "        # Remove all non-numeric characters\n",
    "        salary_str = re.sub(r\"[^\\d]\", \"\", salary_str)\n",
    "        \n",
    "        try:\n",
    "            return int(salary_str)  # Convert to integer if possible\n",
    "        except ValueError:\n",
    "            return salary_str  # Return raw value if conversion fails\n",
    "\n",
    "    return None  # Return None if no salary is found\n",
    "\n",
    "def extract_with_matcher(text):\n",
    "    results = {\"Nombre\": None, \"CC\": None, \"Salario\": None, \"from_tiempo_laborado\": None, \"to_tiempo_laborado\": None}\n",
    "    results[\"Nombre\"] = extract_name(text)\n",
    "    results[\"CC\"] = extract_cc(text)\n",
    "    results[\"Salario\"] = extract_salario(text)\n",
    "    results[\"from_tiempo_laborado\"], results[\"to_tiempo_laborado\"] = extract_tiempo_laborado(text)\n",
    "    return results\n",
    "\n",
    "def process_docs_folder():\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    docs_folder = os.path.join(current_dir, \"docs\")\n",
    "    if not os.path.exists(docs_folder) or not os.path.isdir(docs_folder):\n",
    "        print(f\"Subfolder 'docs' does not exist in the current directory: {current_dir}\")\n",
    "        return\n",
    "\n",
    "    files = [f for f in os.listdir(docs_folder) if f.endswith((\".docx\", \".pdf\", \".png\"))]\n",
    "    if not files:\n",
    "        print(f\"No documents found in the 'docs' subfolder: {docs_folder}\")\n",
    "        return\n",
    "\n",
    "    data = []\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(docs_folder, file_name)\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        if file_name.endswith(\".docx\"):\n",
    "            text = extract_text_from_docx(file_path)\n",
    "        elif file_name.endswith(\".pdf\"):\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "        elif file_name.endswith(\".png\"):\n",
    "            text = extract_text_from_png(file_path)\n",
    "        else:\n",
    "            continue\n",
    "        extracted_data = extract_with_matcher(text)\n",
    "        extracted_data[\"File Name\"] = file_name\n",
    "        data.append(extracted_data)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    output_csv = os.path.join(current_dir, \"extracted_data.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nExtracted data saved to: {output_csv}\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_result = process_docs_folder()\n",
    "    print(\"\\nExtracted DataFrame:\")\n",
    "    print(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/carloscuartas/Library/CloudStorage/Box-Box/MM-TI-Análitica/03. Proyectos/Fraudubot Final/Notebooks\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/carloscuartas/Library/CloudStorage/Box-Box/MM-TI-Análitica/03. Proyectos/Fraudubot Final/Notebooks/docs\n"
     ]
    }
   ],
   "source": [
    "docs_folder = os.path.join(current_dir, \"docs\")\n",
    "print(docs_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/carloscuartas/Library/CloudStorage/Box-Box/MM-TI-Análitica/03. Proyectos/Fraudubot Final/Notebooks/docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carloscuartas/Library/CloudStorage/Box-Box/MM-TI-Análitica/03. Proyectos/Fraudubot Final/myenv/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd /Users/carloscuartas/Library/CloudStorage/Box-Box/MM-TI-Análitica/03. Proyectos/Fraudubot Final/Notebooks/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/carloscuartas/Library/CloudStorage/Box-Box/MM-TI-Análitica/03. Proyectos/Fraudubot Final/Notebooks\n"
     ]
    }
   ],
   "source": [
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mar saccmce\n",
      "ia\n",
      "\n",
      "sistema eneriza 10 émparas de 181Y gor 4\n",
      "/ Dia, Never, comoutedor porttl\n",
      "\n",
      "CERTIFICACION LABORAL\n",
      "\n",
      "ELECTRICOS J Y M certifica que el sefior JUAN CARLOS VARGAS\n",
      "BELTRAN, colombiano, identificado con la cédula de ciudadania No.\n",
      "1.069.742.674 y residente en Fusagasuga, Cundinamarca, laboré para\n",
      "esta empresa desde el 10 de enero del 2018 hasta el 10 de diciembre\n",
      "del 2018, con un contrato de prestacién de servicios, desempefando el\n",
      "cargo de INSTALADOR DE REDES ELECTRICAS.\n",
      "\n",
      "Durante este tiempo, el sefior JUAN CARLOS VARGAS BELTRAN recibia\n",
      "una asignacién mensual a titulo de honorarios de OCHOCIENTOS\n",
      "CATORCE MIL PESOS M/CTE ($814.000), sin un horario definido.\n",
      "\n",
      "En virtud del contrato de prestacién de servicios el sefor JUAN CARLOS\n",
      "VARGAS BELTRAN realizaba instalaciones eléctricas domiciliarias,\n",
      "instalacién de cableado interno e instalaciones de tomas e interruptores.\n",
      "\n",
      "Se expide la presente certificaci6n en Fusagasuga, Cundinamarca, a los\n",
      "11 dias del mes de junio del 2021.\n",
      "\n",
      "MARIBEL BELTRAN CALDERON\n",
      "\n",
      "C.C.\n",
      "\n",
      "Representante legal de ELECTRICOS J YM\n",
      "Cel: 3202397601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'\n",
    "print(pytesseract.image_to_string(Image.open('/Users/carloscuartas/Library/CloudStorage/Box-Box/MM-TI-Análitica/03. Proyectos/Fraudubot Final/Docs2/Carta8.png')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists('/Users/carloscuartas/Library/CloudStorage/Box-Box/MM-TI-Análitica/03. Proyectos/Fraudubot Final/Docs2/Carta8.png'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cedula=1110549584\n",
    "fraude = pd.read_excel(\"../Datos/BASE_PARA_PREDICT.xlsx\")\n",
    "caso = fraude[fraude['CEDULA'] == cedula]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>PRESTACIONES_SOCIALES</th>\n",
       "      <th>COINCIDE_DIRECCION</th>\n",
       "      <th>CONTEO_TELEFONO_BASEFRAUDE</th>\n",
       "      <th>CONTEO_CORREO_BASEFRAUDE</th>\n",
       "      <th>OTRAS_SOLICITUDES</th>\n",
       "      <th>CONTEO_TELEFONO_RECONOCER</th>\n",
       "      <th>CONTEO_EMAIL_RECONOCER</th>\n",
       "      <th>CANT_MORA30_ULT12MESES_HISTORICO</th>\n",
       "      <th>CANT_MORA60_ULT12MESES_HISTORICO</th>\n",
       "      <th>CANT_MORA90_ULT12MESES_HISTORICO</th>\n",
       "      <th>CANT_MORA120_ULT12MESES_HISTORICO</th>\n",
       "      <th>RESULTADO_SCORE</th>\n",
       "      <th>PERSONAS_CARGO</th>\n",
       "      <th>DIFERENCIA_meses_mail</th>\n",
       "      <th>DIFERENCIA_meses_tel</th>\n",
       "      <th>INGRESOS_CONSOLIDADO_smlv</th>\n",
       "      <th>EGRESOS_CONSOLIDADO_smlv</th>\n",
       "      <th>ENDEUDAMIENTO_SMLV</th>\n",
       "      <th>CEDULA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8273</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.84</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>733</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1110549584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10519</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>70.83</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1110549584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10593</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>932</td>\n",
       "      <td>0</td>\n",
       "      <td>379</td>\n",
       "      <td>1110549584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       N  PRESTACIONES_SOCIALES  COINCIDE_DIRECCION  \\\n",
       "0   8273                      1                   0   \n",
       "1  10519                      1                   0   \n",
       "2  10593                      1                   1   \n",
       "\n",
       "   CONTEO_TELEFONO_BASEFRAUDE  CONTEO_CORREO_BASEFRAUDE  OTRAS_SOLICITUDES  \\\n",
       "0                           0                         0                  0   \n",
       "1                           0                         0                  0   \n",
       "2                           0                         0                  1   \n",
       "\n",
       "   CONTEO_TELEFONO_RECONOCER  CONTEO_EMAIL_RECONOCER  \\\n",
       "0                          1                       2   \n",
       "1                          3                       1   \n",
       "2                          2                       2   \n",
       "\n",
       "   CANT_MORA30_ULT12MESES_HISTORICO  CANT_MORA60_ULT12MESES_HISTORICO  \\\n",
       "0                                 0                                 0   \n",
       "1                                 1                                 1   \n",
       "2                                 0                                 0   \n",
       "\n",
       "   CANT_MORA90_ULT12MESES_HISTORICO  CANT_MORA120_ULT12MESES_HISTORICO  \\\n",
       "0                                 0                                  0   \n",
       "1                                 1                                  1   \n",
       "2                                 0                                  0   \n",
       "\n",
       "   RESULTADO_SCORE  PERSONAS_CARGO  DIFERENCIA_meses_mail  \\\n",
       "0            52.84               0                     -1   \n",
       "1            70.83               0                     -1   \n",
       "2            52.58               0                      1   \n",
       "\n",
       "   DIFERENCIA_meses_tel  INGRESOS_CONSOLIDADO_smlv  EGRESOS_CONSOLIDADO_smlv  \\\n",
       "0                     2                        733                         0   \n",
       "1                     1                          0                         0   \n",
       "2                     1                        932                         0   \n",
       "\n",
       "   ENDEUDAMIENTO_SMLV      CEDULA  \n",
       "0                   0  1110549584  \n",
       "1                   0  1110549584  \n",
       "2                 379  1110549584  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp_file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# After the PDF file is uploaded\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mtemp_file_path\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m200\u001b[39m)  \u001b[38;5;66;03m# Read the first 200 bytes\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(content)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'temp_file_path' is not defined"
     ]
    }
   ],
   "source": [
    "# After the PDF file is uploaded\n",
    "with open(temp_file_path, \"rb\") as f:\n",
    "    content = f.read(200)  # Read the first 200 bytes\n",
    "    print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESSDATA_PREFIX is set to: /opt/homebrew/share/\n",
      "Tesseract is located at: /opt/homebrew/bin/tesseract\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from word2number import w2n\n",
    "import spacy\n",
    "import unicodedata\n",
    "from PIL import Image\n",
    "\n",
    "# Set the TESSDATA_PREFIX environment variable\n",
    "os.environ['TESSDATA_PREFIX'] = '/opt/homebrew/share/'  # Adjust accordingly\n",
    "\n",
    "# Set the path to Tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'  # Adjust accordingly\n",
    "\n",
    "# Check if this is being set correctly\n",
    "print(f\"TESSDATA_PREFIX is set to: {os.getenv('TESSDATA_PREFIX')}\")\n",
    "print(f\"Tesseract is located at: {pytesseract.pytesseract.tesseract_cmd}\")\n",
    "\n",
    "# Test with your OCR functions now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
